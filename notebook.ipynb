{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lib.services import config\n",
    "import lib.embedding_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(config['dataset']['path']['train'])\n",
    "train_commnets = train[config['dataset']['features'][0]].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(config['dataset']['path']['test'])\n",
    "test_comments = test[config['dataset']['features'][0]].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First comment:\\n\\t\", train_commnets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Second comment: \", test_comments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train[config['dataset']['labels']].values\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=config['word_embeding']['rows_count'])\n",
    "tokenizer.fit_on_texts(list(train_commnets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenization convert comments to sequence of word indexes. It aboint to:\n",
    "* Get an array of unrepeted words taken of all comments.\n",
    "* Replace each word under a comments to his index.\n",
    "* As result each comment is tranformed to an array of word indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_comments = tokenizer.texts_to_sequences(train_commnets)\n",
    "tokenized_test_comments = tokenizer.texts_to_sequences(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First train comment sequence:\\n\\t\", tokenized_train_comments[0])\n",
    "print(\"First test comment sequence:\\n\\t\", tokenized_test_comments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is necessary take each comment sequence and complet with zeros up to fixes size. As result all comment sequences have same len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pad_sequences(tokenized_train_comments, maxlen=config['word_embeding']['columns_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_set = pad_sequences(tokenized_test_comments, maxlen=config['word_embeding']['columns_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First train fixed size comment sequence:\\n\\t\", train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = utils.build_embedding_matrix(\n",
    "    embedding_file=config['word_embeding']['path'],\n",
    "    rows_count=config['word_embeding']['rows_count'],\n",
    "    columns_count=config['word_embeding']['columns_count'],\n",
    "    word_index=tokenizer.word_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word embedding shape: \", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unrepeated words from all comments: \", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(config['word_embeding']['columns_count'],))\n",
    "\n",
    "x = Embedding(\n",
    "    config['word_embeding']['rows_count'], \n",
    "    config['word_embeding']['columns_count'], \n",
    "    weights=[embedding_matrix]\n",
    ")(model_input)\n",
    "\n",
    "x = Bidirectional(\n",
    "    LSTM(\n",
    "        50, \n",
    "        return_sequences=True, \n",
    "        dropout=0.1, \n",
    "        recurrent_dropout=0.1)\n",
    ")(x)\n",
    "\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "x = Dense(\n",
    "    50, \n",
    "    activation=\"relu\"\n",
    ")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(\n",
    "    6, \n",
    "    activation=\"sigmoid\"\n",
    ")(x)\n",
    "\n",
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_set,\n",
    "    train_labels,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict([test_set], batch_size=1024, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
